===
FAQ
===

RBD
---

qemu-kvm и discard
++++++++++++++++++

И Ceph (RBD) и Qemu умеют в discard/trim/unmap. Это означает, что гостевая ОС
может отправить соответствующий запрос к хранилищу, чтобы сообщить что данные не
нужны. Исходно это было предназначено для SSD-дисков с целью оптимизации
wear-leveling (выравнивание износа). В RBD это позволяет удалить ненужные данные
из кластера и тем самым уменьшить бекфиллинг, размеры снапшотов и др.

Мы знаем, что RBD виртуально делит диски на куски по 4 МБ (по-умолчанию). Каждый
кусок -- это один объект Rados. Соответственно, дискард может либо удалить целиком
один объект (если он не нужен), либо сократить размер (вплоть до нуля). Что он не может
-- так это продискардить середину объекта или начало. Он мог бы просто заполнить нулями,
но нет. Появляется ошибка, правда, не фатальная. В API RBD нет функции для
выяснения размера чанка. Поэтому Qemu не может догадаться какие дискарды можно отправлять
в librbd, а какие закончатся ошибкой. В документации про это ничего не сказано.

Есть костыль для ceph.conf -- параметр ``rbd_skip_partial_discard``. Однако:

* http://tracker.ceph.com/issues/16386
* http://tracker.ceph.com/issues/16869

В связи с чем, лучше проинструктировать Qemu чтобы она сообщила в гостевую об
имеющемся выравнивании в 4 МБ. Тогда ядро гостевой ОС не будет отправлять дискарды
которые невозможно выполнить в RBD.

К сожалению, libvirt напрямую не может указать это для каждого диска персонально.
Но есть костыль:

.. code-block:: xml

   <domain type='kvm' xmlns:qemu='http://libvirt.org/schemas/domain/qemu/1.0'>
     <qemu:commandline>
       <qemu:arg value='-global'/>
       <qemu:arg value='scsi-hd.discard_granularity=4194304'/>
     </qemu:commandline>
     ...
     <disk type='network' device='disk'>
        <!--
            detect_zeroes='on'
            https://libvirt.org/formatdomain.html:

            NB enabling the detection is a compute intensive operation,
            but can save file space and/or time on slow media.

            А ещё это может повлиять на бенчмарки в стиле dd if=/dev/zero ...
            fio использует случайный паттерн.
         -->
        <driver name='qemu' type='raw' cache='writeback' discard='unmap'/>
        ...
     </disk>
   </domain>

.. important::

   ``xmlns:qemu='http://libvirt.org/schemas/domain/qemu/1.0'`` крайне важен.
   Без этой строки ``<qemu:commandline>`` будет проигнорирован. Проверить
   можно повторно отредактировав описание ВЫКЛЮЧЕНОЙ виртуальной машины
   ``virsh edit some-domain``

   Данный фрагмент будет работать только для virtio-scsi. Для IDE аналогично,
   но мне неизвестно как :).

(Пример по ссылке http://docs.ceph.com/docs/master/rbd/qemu-rbd похоже что
устарел и не работает)

Для справки есть ещё такие параметры:

* ``logical_block_size``
* ``physical_block_size``
* ``min_io_size``
* ``opt_io_size``

.. important::

   Discard будет работать только для виртуальных дисковых интерфейсов IDE и
   virtio-scsi. Не путайте virtio и virtio-scsi -- это два совершенно разных
   интерфейса. virtio устарел и более не развивается. В гостевой ОС
   virtio-scsi выглядит как ``/dev/sd*``, а virtio как ``/dev/vd*``.


Их всех можно посмотреть командой ``lsblk`` в гостевой ОС чтобы удостовериться,
что виртуальная машина настроена правильно
(``DISC-GRAN`` равен размеру чанка в RBD):

.. code::

   $ lsblk -D
   NAME   DISC-ALN DISC-GRAN DISC-MAX DISC-ZERO
   sda           0        4M       1G         0
   ├─sda1  4176896        4M       1G         0
   ├─sda2  3145728        4M       1G         0
   └─sda3  3145728        4M       1G         0

   $ lsblk -t
   NAME   ALIGNMENT MIN-IO OPT-IO PHY-SEC LOG-SEC ROTA SCHED    RQ-SIZE  RA WSAME
   sda            0    512      0     512     512    1 deadline     128 128    2G
   ├─sda1         0    512      0     512     512    1 deadline     128 128    2G
   ├─sda2         0    512      0     512     512    1 deadline     128 128    2G
   └─sda3         0    512      0     512     512    1 deadline     128 128    2G


Чтобы это заработало полностью, нужно не только убедиться что эта возможность
появилась на блочном уровне в гостевой ОС, но и чтобы гостевая ОС
использовала эту функцию.

Linux
~~~~~

* ``fstrim -v -a``. Вручную, либо по расписанию (раз в неделю). Рекомендуется.
  не уверен, но в Ubuntu, по-моему, работает из коробки.
* Опции для SWAP-разделов. TODO: расписать какие именно. Есть первичный дискард
  перед подключением, есть включение дискарда во время работы.
* Есть опции при монтировании различных ФС чтобы выполняли discard для данных
  которые стали ненужными (после удаления файлов)
* Команда ``blkdiscard`` для очистки всего устройства либо раздела или тома LVM.

.. warning::

   Говорят, что опции монтирования и аналогичные опции для SWAP-раздела понижают
   производительность. С другой стороны, массивный fstrim по расписанию может
   дать непредвиденные проседания IO в гостевой ОС.

Windows
~~~~~~~

TODO: всё работает из коробки как-то само собой. На старых версиях можно включить
через реестр. Как посмотреть ? Как форсировано прочистить ?

Настоятельно рекомендуется установить дополнения в гостевую ОС:

* https://fedoraproject.org/wiki/Windows_Virtio_Drivers
* https://www.linux-kvm.org/page/WindowsGuestDrivers/Download_Drivers

Иначе придётся довольствоваться только IDE, а это сильно меньшая производительность.


Недорасписанное
+++++++++++++++

* опции для рбд образов типа фастдифф
* бага с удалением снапшотов созданных ранними версиями
* откат к снапшоту крайне медленный (как он работает) и что без дедупликации по сравнению со старыми
  объектам

* Виды кеширования в квм - дока от сусе где демелиоратор сказал что он не прав.
  И описание что есть потеря данных при вырубания питания.
  https://www.spinics.net/lists/ceph-users/msg15983.html
  http://docs.ceph.com/docs/master/rbd/qemu-rbd/#qemu-cache-options

* скруб еррор -- как понять хотябы какой это образ.
* как бекапить :)
* в рбд сразу после снапшота будут наблюдаться тормоза так как 4-мб объекты будут копироваться целиком даже при записи одного сектора.
* оборванное удаление образа. как доудалить остатки.
* преобразование в qcow2 и обратно. сжатие qcow2. перенос в другой пул средством qemu-img. хотя более быстро -- на уровне rados.
* Ядерный драйвер RBD не умеет во много опций. в частности, фастдифф. Варианты -- FUSEmount -- каждый файл это образ. либо NBD.
* iscsi
* qemu-nbd vs rbd-nbd
* Перенос образов между пулами и копирование образов: рекомендуется qemu-img версии более 2.9.

  .. image:: _static/qemu-img-bandwith.jpg
     :alt: График пропускной способности

  https://github.com/qemu/qemu/commit/2d9187bc65727d9dd63e2c410b5500add3db0b0d и описание опций.

* Сделав снапшот хотябы одного образа, сделать снапшот пула уже не получится. Узнать бы почему.


Переход на Luminous
-------------------

При переходе на Luminous нужно сделать не только то что в инструкции (ссылка) но ещё и ряд действий.
В т.ч. проблемы с удалением старых снапшотов.


CephFS
------

Хранить образы виртуалок на CephFS -- полный маразм.

Типичные крутилки/инструкции
----------------------------

* Минимизация влияния бекфиллов и рекавери на ИО (и описать в чём опасность)
* ревеигхт бай утилизейшен (новые ребалансер в Люминоусе?)

Как удалить OSD
+++++++++++++++

Для примера будем удалять `osd.42`.

#. ``ceph osd out osd.42``. Эта команда заставит Ceph перенести все данные с
   этого диска на другие диски без даже вре́менного понижения количества реплик.
#. Мониторить ``ceph osd safe-to-destroy``.
#. На ноде: ``sudo systemctl stop ceph-osd@42``.
#. ``ceph osd purge osd.42``.

Дальнейшие операцию производятся на ноде под правами root:

#. Посмотреть и запомнить вывод ``lsblk -f``. Пригодится далее для ``wipefs``.
#. Посмотреть и запомнить ``readlink -f /var/lib/ceph/osd/ceph-42/*``
   (Пригодится для удаления журнального раздела если он выносной).
#. ``umount /var/lib/ceph/osd/ceph-42``.
#. ``rmdir /var/lib/ceph/osd/ceph-42``.
#. ``wipefs -a /dev/{data-disk-partition(s)}``. см. сохранённый вывод ``lsblk``.
#. ``wipefs -a /dev/{data-disk}``. см. сохранённый вывод ``lsblk``.
#. Если выносной журнал/бд: ``fdisk /dev/{journal-disk}``, удалить
   соответствующий раздел. Современный fdisk умеет работать с GPT.
   какой именно раздел -- см. сохранённый вывод ``readlink``.
#. ``partprobe /dev/{journal-disk}``. fdisk не умеет говорить ядру о применении
   измененной таблицы разделов если диск используется (например, под другие
   журналы/бд на этом же диске.
#. Перед извлечением диска физически на лету выполнить:
   ``echo 1 > /sys/block/{data-disk}/device/delete``.

Как работает
------------
* почему дедупликация крайне затруднена в архитектуре Ceph
* в файлсторе всё полностью пишется в журнал. один врайт превращается в два сисколла врайт
  - один в журнал (с синком) и один в основное хранилище. Но основное хранилище фсинкается
  время от времени. Запись в журнал линейная, а в основное хранилище рандомная. При записи
  в хранилище поможет параллельность которую может диск (например, NCQ). при записи в журнал
  параллельность не используется. поэтому для файлстора надо бенчить именно *так*.
  WAL используется как writeback-cache по-сути.
* при выносе журнала или БД на отдельный диск теряется возможность перевставлять диски в
  другой нод. При старте ОСД (бай дефолт есть параметр) обновляет себя в крушмапе.
* При потере журнала вседиски на него зааттаченные превращаются в труху
* При потере данных всех мониторов теряется весь кластер.
* Нужно использовать именно три реплики потому что если две - то при скраб ерроре не понятно
  кому верить
* запись и чтение делается исключительно с мастера в актинг сете. При записи данные
  отправляются на мастер осд а он по кластер-сети  отправляет параллельно на два слейва.
  on_safe-коллбэк клиента вызывается когда данные записались в WAL на всех репликах.
  Должидания прописывания в основное хранилище в принципе нет. Есть коллбэк когда данные
  есть в памяти на всех трёх репликах.
* бкеш врёт относительно ротейшионал и цеф использует не те настройки. Бкеш writeback
  (кеширование записи) не нужен потому что с файлстором это делается через WAL, а с
  блюстором есть опция по записи даже больших запросов в БД которую нужно вынести на ССД.
  С чтением тоже не нужен потому что:

  #. виртуалки с рбд и так не плохо кешируют то что уже читали

  #. запись потребляет в 3 раза больше иопсов (размер пула=3). а на самом деле ещё больше по
     причине двойной записи и даже ещё больше если это файлстор. Чтение требует один-в-один.
     поэтому цеф на чтение хорош.

  #. Нормальный кеш делает через тиеринг в цефе (но это не точно).

* Описание цифр в ceph -s. откуда берутся цифры и что они означают.
* Как посчитать реальную вместимость кластера. мин. загруженность осд.
* сколько должен давать кластер иопсов и мегабайтов в секунду на чтение и на запись.
  какие паттерны использования и параллельность.
* ceph-deploy требует GPT. Размер журнала и БД надо выставлять.
* Инструкцию по перемещению журнала на другое место для файлстора. и факт что это невозможно для блюстора.
* понимание, что ИО одного и того же обжекта (или 4-мб-блока в рбд) никак не распараллеливается магически.
  и оно будет равно иопсам журнала или осн. хранилища.
* почему мелкие объекты плохо в радосе и большие тоже плохо.
* почему при убирании диска надо сначала сделать цеф осд аут, а не просто вырубить диск.
* для более быстрой перезагрузки используйте kexec. systemctl kexec. однако с кривым железом может
  не работать (сетёвки и рейды/хба).
* https://habrahabr.ru/post/313644/
* почему size=3 min_size=1 (size 3/1) моежт привести к проблемам.
* Каждая пг устанавливает свой кворум таким образом много
* ссылка на калькулятор количества ПГ. почему много пг плохо и мало пг тоже плохо.

  * http://ceph.com/pgcalc

  * если мало - то неравномерность, потенциально не все осд могут быть заюзаны.

  * если много - юсадж памяти, перегрузка сети




Бенчмаркинг
-----------

* Как бенчмаркить сам цеф и рбд. какие типовые кейсы. говорят, фио врёт про рбд
  (надо исходники посмотреть рбд драйвера).
* что иопсы равны самым медленным иопсам серди актинг сета.
* как бенчить радос. нужно сопоставить рассчетное и фактическое. ибо всегда можно создать
  нагрузку которая задосит кластер.
* RBD надо бенчить на зааллокейченном диске.

Мониторинг
----------

* два вида экспортеров под прометеус
* мониторить температуры, свап, иопсы (латенси) дисков

Сеть
----

* что бек сеть надо точно 10 гигабит. привести расчёты.
* Отключить оффлоадинг (и как проверить помогло ли) - меряем RTT внутри TCP.
* джамбофреймы могут помочь но не особо. сложности со свичами обычно.
* мониторить состояние линка. оно иногда самопроизвольно падает с гигабита на 100 мегабит.
* Тюнинг TCP/IP - отключать контрак

Диски
-----

* запрещено использовать аппаратные рейды. имеется в виду в режиме рейда. Опасность обмана
  фсинков (например, включенный врайтбек на рейде без BBU). В рейдах цеф не нуждается в принципе.
  в апп. рейде пока диск не просинкается рейд дегрейдед. Уж лучше цеф сам позаботится о репликах.
* Акустик, хпа, паверсейвинг, настроить автотесты по смарту.
* отдискардить ссд перед использованием.
* fstrim -v -a (filestore on ssd), blkdiscard on LVM/Partition.
* мониторить смарт
* как бенчить - ссд и разного рода коммерческий обман. деградация изза недискарда - надо дать
  продыхнуть, некоторое количество быстрых ячеек и тиринг на них. суперкапазиторы.
* бенчмаркинг несколько дисков одновременно ибо контроллеры.
* на ссд обновлять прошивки критично важно. ещё про блеклисты в ядрах насчёт багов.
* дискард на них медленный, поэтому лучше оставить продискарденную область и этого достаточно.
* жеоательно не ставить одинаковые диски с одинаковым юсаджем - ибо умрут скорее всего одновременно
  ибо нагрузка примерно одинаковая.
* Диск шедулеры
* имхо магнитные сас-диски не нужны. их возможности не будут задействованы для получения преимущества
  перед сата. Сата 12 гбит для магнитных дисков не нужен. Для магнитных (7200 оборотов)
  даже сата2 (3 гбит ~ 300 мб.сек) хватит.
* убедиться что диски подключены как сата6.
* чего ожидать от бенчмаркинга. реальная таблица с реальными моделями.
* при бенчмаркинге ссд может оказаться что уперлись в контроллер а не в диск.

Процессоры и память
-------------------

* ECC - потому что сбой в памяти мастер-осд в актинг сете приведёт к повреждению данных
  даже если это BlueStore со своим крк - данные могут быть испорчены до подсчёта крк и распространены
  по слейвам.
* говернор и паверсейв.
* CRC32 аппаратное в блюсторе (и в месенджере не с блюстором?)
* гипертрединг нинужен. потому что это просто доп-набор регистров. В цефе по идее нет цпу-боунд задач
  есть крк32 но оно реализуется через спец команду в sse4.3 а такой блок емнип один на ядро.
  при сжатии в блюсторе может иметь значение однако.
* ramspeed = ramsmp
* cpuburn
* i7z, powertop
* cpupower frequency-info, how to set governor (+permanently)
